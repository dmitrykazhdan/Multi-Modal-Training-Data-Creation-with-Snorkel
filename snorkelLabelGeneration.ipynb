{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add relevant import statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.learning import GenerativeModel\n",
    "from scipy import sparse\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "from shutil import copyfile\n",
    "import operator\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Package data-handling code into a class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader(object):\n",
    "    def __init__(self, data_path):\n",
    "    \n",
    "        sub_dirs = [name for name in os.listdir(data_path) if os.path.isdir(os.path.join(data_path, name))]\n",
    "        self.files = [] # Array of filenames\n",
    "        self.truths = {} # Dictionary holding a mapping from a filename to its class\n",
    "        self.data_path = data_path\n",
    "\n",
    "        for sub_dir in sub_dirs:\n",
    "            # Extract all image files from the given directory\n",
    "            sub_dir_path = data_path + sub_dir + \"/images/\"\n",
    "            sub_dir_files = [sub_dir_path + f for f in os.listdir(sub_dir_path) if f[-3:] == 'jpg']\n",
    "            self.files += sub_dir_files\n",
    "            \n",
    "            for sub_dir_file in os.listdir(sub_dir_path):\n",
    "                if sub_dir_file in self.truths:\n",
    "                    raise ValueError(\"File appearing twice in different directories\")\n",
    "                else:\n",
    "                    self.truths[sub_dir_file] = sub_dir\n",
    "        \n",
    "        self.files.sort()     \n",
    "        self.train_num = len(self.files)\n",
    "\n",
    "    # Retrieve contents of a text file, given its index into the 'files' array\n",
    "    def get_file_contents(self, i):\n",
    "        \n",
    "        f_name = self.get_text_filename(i)\n",
    "        \n",
    "        if not os.path.isfile(f_name):\n",
    "            print(\"File not found: \", f_name)\n",
    "            return \"\"\n",
    "        \n",
    "        file = open(f_name)\n",
    "        contents = file.read()\n",
    "        file.close()\n",
    "        return contents\n",
    "    \n",
    "    # Retrieve the full filename of an image file, given its index into the 'files' array\n",
    "    def get_img_filename(self, i):\n",
    "        if i >= len(self.files):\n",
    "            raise ValueError('Illegal file index')\n",
    "            \n",
    "        f_name = self.files[i]\n",
    "        return f_name\n",
    "    \n",
    "    \n",
    "    # Retrieve the full filename of a text file, given its index into the 'files' array\n",
    "    def get_text_filename(self, i):\n",
    "        img_filename = self.get_img_filename(i)\n",
    "        parent_dir = os.path.basename(os.path.dirname(os.path.dirname((img_filename))))\n",
    "        text_name = os.path.basename(img_filename)[:-3] + 'txt'\n",
    "        return self.data_path + parent_dir + \"/text/\" + text_name \n",
    "    \n",
    "    # Retrieve the index of a file, given its full name\n",
    "    def find_fileindex(self, f_name):\n",
    "        for i, file in enumerate(self.files):\n",
    "            if os.path.basename(file) == f_name:\n",
    "                return i\n",
    "        return -1\n",
    "    \n",
    "    # Retrieve the hand-label of a file, given its index into the 'files' array\n",
    "    def get_truth(self, i):\n",
    "        f_name = self.get_img_filename(i)\n",
    "        return self.truths[os.path.basename(f_name)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load training data into the data loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num files:  1795\n"
     ]
    }
   ],
   "source": [
    "loader = DataLoader('path-to-parsed-data')\n",
    "\n",
    "print(\"Num files loaded: \", loader.train_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create keyword-based functions for text labelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mentions_fire(text_content):\n",
    "    keywords = ['fire', 'flame', 'smoke', 'burn', 'burning', 'ash', 'forest', 'gas', 'explosion']\n",
    "    \n",
    "    for keyword in keywords:\n",
    "        if keyword in text_content:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "def mentions_damaged_flood(text_content):\n",
    "    keywords = ['flood', 'water', 'rain', 'disaster', 'damage',]\n",
    "    \n",
    "    for keyword in keywords:\n",
    "        if keyword in text_content:\n",
    "            return 2\n",
    "    return 0\n",
    "\n",
    "def mentions_human_damage(text_content):\n",
    "    keywords = ['death', 'dead', 'victim', 'loss', 'kill', 'injured', \n",
    "                'injury', 'war', 'refugee', 'crime', 'attack', 'crisis', 'explosion']\n",
    "    \n",
    "    for keyword in keywords:\n",
    "        if keyword in text_content:\n",
    "            return 3\n",
    "    return 0\n",
    "\n",
    "def mentions_damaged_infrastructure(text_content):\n",
    "    keywords = ['wreck', 'collapse', 'broken', 'bridge', 'building', \n",
    "                'storm', 'disaster', 'damage', 'destroy', 'destruction']\n",
    "    \n",
    "    for keyword in keywords:\n",
    "        if keyword in text_content:\n",
    "            return 4\n",
    "    return 0\n",
    "\n",
    "def mentions_damaged_nature(text_content):\n",
    "    keywords = ['nature', 'animals', 'wildlife', 'habitat', 'storm', 'destruction',  \n",
    "                'environment', 'tree', 'drought', 'landslide', 'forest', 'tornado']\n",
    "    \n",
    "    for keyword in keywords:\n",
    "        if keyword in text_content:\n",
    "            return 5\n",
    "    return 0\n",
    "\n",
    "def non_damage(text_content):\n",
    "  \n",
    "    bad_words = [\n",
    "                'burn', 'wreck', 'collapse', 'disaster', 'damage', 'destroy', 'destruction', \n",
    "                'death', 'dead', 'victim', 'kill', 'injured', \n",
    "                'injury', 'war', 'refugee', 'attack', 'crisis',\n",
    "                'storm', 'blizzard', 'drought', 'tornado', 'fire', 'landslide', 'earthquake', 'volcano',\n",
    "                'eruption', 'flood', 'tsunami']\n",
    "\n",
    "    for keyword in bad_words:\n",
    "        if keyword in text_content:\n",
    "            return 0\n",
    "    return 6\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a primitive matrix using these functions and files (as described in the Snorkel tutorials)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_primitives(loader):\n",
    "    m = 6 # number of primitives\n",
    "    primitive_mtx = np.zeros((loader.train_num,m))\n",
    "\n",
    "    for i in range(loader.train_num):\n",
    "        primitive_mtx[i,0] = mentions_fire(loader.get_file_contents(i))\n",
    "        primitive_mtx[i,1] = mentions_damaged_flood(loader.get_file_contents(i))\n",
    "        primitive_mtx[i,2] = mentions_human_damage(loader.get_file_contents(i))\n",
    "        primitive_mtx[i,3] = mentions_damaged_infrastructure(loader.get_file_contents(i))\n",
    "        primitive_mtx[i,4] = mentions_damaged_nature(loader.get_file_contents(i))\n",
    "        primitive_mtx[i,5] = non_damage(loader.get_file_contents(i))\n",
    "               \n",
    "    return primitive_mtx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "primitive_mtx = create_primitives(loader)\n",
    "\n",
    "p_keys = {\n",
    "    'has_fire': primitive_mtx[:,0],\n",
    "    'has_flood': primitive_mtx[:,1],\n",
    "    'has_human_damage': primitive_mtx[:,2],\n",
    "    'has_damaged_infrastructure': primitive_mtx[:,3],\n",
    "    'has_damaged_nature': primitive_mtx[:,4],\n",
    "    'has_non_damage': primitive_mtx[:,5]\n",
    "   }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the text-labelling functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LF_has_fire(has_fire):\n",
    "    return has_fire\n",
    "\n",
    "def LF_has_flood(has_flood):\n",
    "    return has_flood\n",
    "\n",
    "def LF_has_human_damage(has_human_damage):\n",
    "    return has_human_damage\n",
    "\n",
    "def LF_has_damaged_infrastructure(has_damaged_infrastructure):\n",
    "    return has_damaged_infrastructure\n",
    "\n",
    "def LF_has_damaged_nature(has_damaged_nature):\n",
    "    return has_damaged_nature\n",
    "\n",
    "def LF_has_non_damage(has_non_damage):\n",
    "    return has_non_damage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_fns = [LF_has_fire, LF_has_flood, LF_has_human_damage, LF_has_damaged_infrastructure, \n",
    "         LF_has_damaged_nature, LF_has_non_damage]\n",
    "\n",
    "L = np.zeros((len(L_fns),loader.train_num)).astype(int)\n",
    "\n",
    "for i in range(loader.train_num):\n",
    "    L[0,i] = L_fns[0](p_keys['has_fire'][i])\n",
    "    L[1,i] = L_fns[1](p_keys['has_flood'][i])\n",
    "    L[2,i] = L_fns[2](p_keys['has_human_damage'][i])\n",
    "    L[3,i] = L_fns[3](p_keys['has_damaged_infrastructure'][i])\n",
    "    L[4,i] = L_fns[4](p_keys['has_damaged_nature'][i])\n",
    "    L[5,i] = L_fns[5](p_keys['has_non_damage'][i])\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_train = sparse.csr_matrix(L.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the generative model using these labelling functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferred cardinality: 6\n"
     ]
    }
   ],
   "source": [
    "gen_model = GenerativeModel()\n",
    "\n",
    "gen_model.train(L.T, epochs=100, decay=0.95, step_size= 0.01/ L.shape[1], reg_param=1e-6)\n",
    "train_marginals = gen_model.marginals(L_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for selecting the label with highest probability\n",
    "def get_best_prob(probabilities):\n",
    "    largest_prob = max(probabilities)\n",
    "    largest_indices = []\n",
    "    \n",
    "    for index in range(len(probabilities)):\n",
    "        if probabilities[index] == largest_prob:\n",
    "            largest_indices.append(index)\n",
    "    \n",
    "    # If there is a single largest value, return it\n",
    "    if len(largest_indices) == 1:\n",
    "        return largest_indices[0]\n",
    "    \n",
    "    # Otherwise randomly select a lable from the labels with the largest probability\n",
    "    else:\n",
    "        return random.choice(largest_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the generated labels to the ground truths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('damaged_nature__damaged_infrastructure', 1), ('flood__damaged_infrastructure', 1), ('human_damage__flood', 1), ('fires__damaged_infrastructure', 2), ('non_damage__flood', 4), ('non_damage__damaged_nature', 4), ('flood__damaged_nature', 5), ('non_damage__fires', 5), ('damaged_nature__fires', 11), ('human_damage__non_damage', 11), ('damaged_infrastructure__fires', 15), ('damaged_infrastructure__damaged_nature', 17), ('fires__non_damage', 20), ('damaged_nature__human_damage', 21), ('flood__human_damage', 24), ('fires__flood', 27), ('damaged_infrastructure__non_damage', 28), ('flood__non_damage', 29), ('fires__human_damage', 31), ('damaged_infrastructure__flood', 42), ('damaged_infrastructure__human_damage', 43), ('non_damage__human_damage', 44), ('damaged_nature__non_damage', 53), ('damaged_nature__flood', 54)]\n",
      "% of correct predictions:  72.53481894150417\n"
     ]
    }
   ],
   "source": [
    "def compare_training_data(train_marginals):\n",
    "    \n",
    "    labels = ['fires', 'flood', 'human_damage', 'damaged_infrastructure', 'damaged_nature', 'non_damage']\n",
    "\n",
    "    count = 0\n",
    "    failures = {}\n",
    "\n",
    "    for i in range(loader.train_num):\n",
    "        predicted = labels[get_best_prob(train_marginals[i])]\n",
    "        truth = loader.get_truth(i)\n",
    "\n",
    "        if (truth != predicted):\n",
    "            key = truth + \":\" + predicted\n",
    "            if key in failures:\n",
    "                failures[key] += 1\n",
    "            else:\n",
    "                failures[key] = 1\n",
    "\n",
    "            count += 1\n",
    "\n",
    "    sorted_x = sorted(failures.items(), key=operator.itemgetter(1))\n",
    "    print(sorted_x)\n",
    "    print(\"% of correct predictions: \", str((1 - count/loader.train_num) * 100))\n",
    "    \n",
    "compare_training_data(train_marginals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create clustering Labelling Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_paths = ['./clustering_1/',\n",
    "                  './clustering_2/',\n",
    "                  './clustering_3/',\n",
    "                  './clustering_4/']\n",
    "\n",
    "# In this case, the foldername of the cluster is the labelling of its elements\n",
    "def get_foldername(filename, clusters_path):   \n",
    "    sub_dirs = [name for name in os.listdir(clusters_path) if os.path.isdir(os.path.join(clusters_path, name))]\n",
    "\n",
    "    for sub_dir in sub_dirs:\n",
    "        sub_dir_path = os.path.join(clusters_path, sub_dir)\n",
    "        sub_sub_dirs = [name for name in os.listdir(sub_dir_path) if os.path.isdir(os.path.join(sub_dir_path, name))]\n",
    "\n",
    "        for sub_sub_dir in sub_sub_dirs:\n",
    "            sub_sub_dir_path=os.path.join(sub_dir_path, sub_sub_dir)\n",
    "\n",
    "            files = os.listdir(sub_sub_dir_path)\n",
    "\n",
    "            if os.path.basename(filename) in files:\n",
    "                return sub_sub_dir_path\n",
    "    return \"None\"\n",
    "\n",
    "def clustering(filename, similarity_level):\n",
    "    \n",
    "    # Here we assume the image filename is located in the cluster folder \n",
    "    # that is named with one of the class labels\n",
    "    clusters_path = clusters_paths[similarity_level]\n",
    "    cluster_folder = get_foldername(filename, clusters_path)\n",
    "\n",
    "    if cluster_folder == \"None\": \n",
    "        return 0\n",
    "    else:\n",
    "        return cluster_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0.0: 1259, 1.0: 23, 2.0: 26, 3.0: 99, 4.0: 84, 5.0: 43, 6.0: 261}\n"
     ]
    }
   ],
   "source": [
    "def create_primitives(loader):\n",
    "    m = 10 # number of primitives\n",
    "    primitive_mtx = np.zeros((loader.train_num,m))\n",
    "\n",
    "    for i in range(loader.train_num):\n",
    "        primitive_mtx[i,0] = mentions_fire(loader.get_file_contents(i))\n",
    "        primitive_mtx[i,1] = mentions_damaged_flood(loader.get_file_contents(i))\n",
    "        primitive_mtx[i,2] = mentions_human_damage(loader.get_file_contents(i))\n",
    "        primitive_mtx[i,3] = mentions_damaged_infrastructure(loader.get_file_contents(i))\n",
    "        primitive_mtx[i,4] = mentions_damaged_nature(loader.get_file_contents(i))\n",
    "        primitive_mtx[i,5] = non_damage(loader.get_file_contents(i))\n",
    "        primitive_mtx[i,6] = clustering(loader.get_img_filename(i), 0)\n",
    "        primitive_mtx[i,7] = clustering(loader.get_img_filename(i), 1)\n",
    "        primitive_mtx[i,8] = clustering(loader.get_img_filename(i), 2)\n",
    "        primitive_mtx[i,9] = clustering(loader.get_img_filename(i), 3)\n",
    "        \n",
    "    return primitive_mtx\n",
    "\n",
    "primitive_mtx = create_primitives(loader)\n",
    "\n",
    "p_keys = {\n",
    "    'has_fire': primitive_mtx[:,0],\n",
    "    'has_flood': primitive_mtx[:,1],\n",
    "    'has_human_damage': primitive_mtx[:,2],\n",
    "    'has_damaged_infrastructure': primitive_mtx[:,3],\n",
    "    'has_damaged_nature': primitive_mtx[:,4],\n",
    "    'has_non_damage': primitive_mtx[:,5],\n",
    "    'has_cluster' :  primitive_mtx[:,6],\n",
    "    'has_cluster_2' :  primitive_mtx[:,7],\n",
    "    'has_cluster_3' :  primitive_mtx[:,8],\n",
    "    'has_cluster_4' :  primitive_mtx[:,9]\n",
    "   }\n",
    "\n",
    "def LF_has_cluster(has_cluster):\n",
    "    return has_cluster\n",
    "\n",
    "def LF_has_cluster_2(has_cluster):\n",
    "    return has_cluster\n",
    "\n",
    "def LF_has_cluster_3(has_cluster):\n",
    "    return has_cluster\n",
    "\n",
    "def LF_has_cluster_4(has_cluster):\n",
    "    return has_cluster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_fns = [LF_has_fire, LF_has_flood, LF_has_human_damage, LF_has_damaged_infrastructure, \n",
    "         LF_has_damaged_nature, LF_has_non_damage, LF_has_cluster, LF_has_cluster_2, LF_has_cluster_3,\n",
    "        LF_has_cluster_4]\n",
    "\n",
    "L = np.zeros((len(L_fns),loader.train_num)).astype(int)\n",
    "\n",
    "for i in range(loader.train_num):\n",
    "    L[0,i] = L_fns[0](p_keys['has_fire'][i])\n",
    "    L[1,i] = L_fns[1](p_keys['has_flood'][i])\n",
    "    L[2,i] = L_fns[2](p_keys['has_human_damage'][i])\n",
    "    L[3,i] = L_fns[3](p_keys['has_damaged_infrastructure'][i])\n",
    "    L[4,i] = L_fns[4](p_keys['has_damaged_nature'][i])\n",
    "    L[5,i] = L_fns[5](p_keys['has_non_damage'][i])\n",
    "    L[6,i] = L_fns[6](p_keys['has_cluster'][i])\n",
    "    L[7,i] = L_fns[7](p_keys['has_cluster_2'][i])\n",
    "    L[8,i] = L_fns[8](p_keys['has_cluster_3'][i])\n",
    "    L[9,i] = L_fns[9](p_keys['has_cluster_4'][i])\n",
    "  \n",
    "L_train = sparse.csr_matrix(L.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the generative model and compare to ground truths (if all is well- should improve over the previous result)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferred cardinality: 6\n"
     ]
    }
   ],
   "source": [
    "gen_model = GenerativeModel()\n",
    "\n",
    "gen_model.train(L.T, epochs=100, decay=0.95, step_size= 0.01/ L.shape[1], reg_param=1e-6)\n",
    "train_marginals = gen_model.marginals(L_train)\n",
    "\n",
    "compare_training_data(train_marginals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the Snorkel-generated labelled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write training data\n",
    "def overwrite_dir(dir_path):\n",
    "    if os.path.exists(dir_path):\n",
    "        shutil.rmtree(dir_path)\n",
    "    os.makedirs(dir_path)\n",
    "\n",
    "img_folder = ''\n",
    "text_folder = ''\n",
    "combined_folder = ''\n",
    "\n",
    "overwrite_dir(img_folder)\n",
    "overwrite_dir(text_folder)\n",
    "overwrite_dir(combined_folder)\n",
    "\n",
    "labels = ['fires', 'flood', 'human_damage', 'damaged_infrastructure', 'damaged_nature', 'non_damage']\n",
    "\n",
    "for label in labels:\n",
    "    os.makedirs(img_folder + label)\n",
    "    os.makedirs(text_folder + label)\n",
    "    os.makedirs(combined_folder + label)\n",
    "    os.makedirs(combined_folder + label + \"/images\")\n",
    "    os.makedirs(combined_folder + label + \"/text\")\n",
    "    \n",
    "\n",
    "for i in range(loader.train_num):\n",
    "    predicted_label = labels[get_best_prob(train_marginals[i])]\n",
    "    \n",
    "    img_filename = loader.get_img_filename(i)\n",
    "    dst = img_folder + predicted_label + \"/\" + os.path.basename(img_filename)\n",
    "    copyfile(img_filename, dst)\n",
    "    dst = combined_folder + predicted_label + \"/images/\" + os.path.basename(img_filename)\n",
    "    copyfile(img_filename, dst)\n",
    "    \n",
    "    text_filename = loader.get_text_filename(i)\n",
    "    dst = text_folder + predicted_label + \"/\" + os.path.basename(text_filename)\n",
    "    copyfile(text_filename, dst)\n",
    "    dst = combined_folder + predicted_label + \"/text/\" + os.path.basename(text_filename)\n",
    "    copyfile(text_filename, dst)    \n",
    "\n",
    "   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (snorkel)",
   "language": "python",
   "name": "snorkel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
